# NhaDat from LayDuLieu.com using LGBM & XGBoost
Small Project for FTU's Graduation Requirement

1. crawl.py: The data is crawled through LayDuLieu.com, turning into a csv file (100_pages). Actually, personally I have a "9000_pages" file, which is too heavy to upload
![image](https://user-images.githubusercontent.com/62114168/121917658-8876b400-cd5f-11eb-8714-6865795b5fcb.png)

2. "HN.csv" and "HCM.csv" are housing data from Hanoi and HoChiMinh

3. NhaDat_1.ipynb: for getting "HN.csv" and "HCM.csv" files from "9000_pages" file

4. NhaDat_2.ipynb: For Hanoi machine learning file + HCM below
![image](https://user-images.githubusercontent.com/62114168/121918067-e73c2d80-cd5f-11eb-8481-2d52d6d2dd57.png)

This is just a personal project. I do not own any data or source code here.
Thank you and hopefully you enjoy the code
![image](https://user-images.githubusercontent.com/62114168/121918328-27031500-cd60-11eb-93f7-2a5b9fb3b365.png)
