# NhaDat from LayDuLieu.com using LGBM & XGBoost
Small Project for FTU's Graduation Requirement

crawl.py: The data is crawled through LayDuLieu.com, turning into a csv file (100_pages). Actually, personally I have a "9000_pages" file, which is too heavy to upload
![image](https://user-images.githubusercontent.com/62114168/121917658-8876b400-cd5f-11eb-8714-6865795b5fcb.png)

"HN.csv" and "HCM.csv" are housing data from Hanoi and HoChiMinh

NhaDat_1.ipynb: for getting "HN.csv" and "HCM.csv" files from "9000_pages" file

NhaDat_2.ipynb: For Hanoi machine learning file
